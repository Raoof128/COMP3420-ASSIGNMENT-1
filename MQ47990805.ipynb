{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3981b5",
   "metadata": {},
   "source": [
    "# CIFAR-10 Image Classification: CNNs vs Transfer Learning\n",
    "# COMP3420 Assignment 1\n",
    "# Student ID: [47990805]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42ef9ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# IMPORTS AND SETUP\n",
    "# =============================================================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import models\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from collections import Counter, defaultdict\n",
    "import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Device and reproducibility setup\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CIFAR-10 configuration\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf210a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 1: PREPARE DATA SUBSET (4 marks)\n",
    "# =============================================================================\n",
    "def create_balanced_subset(dataset, samples_per_class=1000, seed=42):\n",
    "    \"\"\"Create balanced subset with 1000 images per class\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample randomly from each class\n",
    "    selected_indices = []\n",
    "    for class_idx, indices in class_indices.items():\n",
    "        sampled = np.random.choice(indices, size=samples_per_class, replace=False)\n",
    "        selected_indices.extend(sampled.tolist())\n",
    "    \n",
    "    np.random.shuffle(selected_indices)\n",
    "    subset = Subset(dataset, selected_indices)\n",
    "    \n",
    "    # Verify balance\n",
    "    class_counts = Counter()\n",
    "    for idx in subset.indices:\n",
    "        _, label = subset.dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    print(\"Balanced subset created:\")\n",
    "    for class_idx, count in sorted(class_counts.items()):\n",
    "        print(f\"  {CIFAR10_CLASSES[class_idx]}: {count} samples\")\n",
    "    \n",
    "    return subset\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load CIFAR-10 with proper transforms\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "    \n",
    "    full_trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=test_transform)\n",
    "    \n",
    "    return full_trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06295ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2: CUSTOM CNN MODEL (5 marks)\n",
    "# =============================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN with 4+ conv layers, batch norm, dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.3),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((2, 2))\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 2 * 2, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 3: MOBILENETV2 TRANSFER LEARNING (4 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def create_mobilenetv2(num_classes=10, pretrained=True):\n",
    "    \"\"\"Create MobileNetV2 adapted for CIFAR-10\"\"\"\n",
    "    model = models.mobilenet_v2(pretrained=pretrained)\n",
    "    \n",
    "    # Freeze early layers for transfer learning\n",
    "    if pretrained:\n",
    "        for param in model.features[:-3].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Modify classifier for CIFAR-10\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(num_features, 256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Initialize new classifier layers\n",
    "    for m in model.classifier.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 4: TRAINING FUNCTION (4 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=20, lr=0.001, weight_decay=1e-4):\n",
    "    \"\"\"Modular training function for both models\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.4f}')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 5: MODEL EVALUATION (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Evaluating'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f} ({correct}/{total})')\n",
    "    \n",
    "    return accuracy, np.array(all_predictions), np.array(all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 6: CONFUSION MATRICES (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"Plot confusion matrix with proper labeling\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training loss and accuracy\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], 'b-')\n",
    "    ax1.set_title(f'{model_name} - Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, [acc*100 for acc in history['train_acc']], 'b-')\n",
    "    ax2.set_title(f'{model_name} - Training Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CIFAR-10 CLASSIFICATION EXPERIMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    full_trainset, testset = load_datasets()\n",
    "    train_subset = create_balanced_subset(full_trainset, SAMPLES_PER_CLASS)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Train Custom CNN\n",
    "    print(\"\\n2. Training Custom CNN...\")\n",
    "    custom_cnn = CustomCNN().to(device)\n",
    "    custom_cnn, custom_history = train_model(custom_cnn, train_loader, NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Train MobileNetV2\n",
    "    print(\"\\n3. Training MobileNetV2...\")\n",
    "    mobilenet = create_mobilenetv2().to(device)\n",
    "    mobilenet, mobilenet_history = train_model(mobilenet, train_loader, NUM_EPOCHS, LEARNING_RATE*0.1)\n",
    "    \n",
    "    # Evaluate both models\n",
    "    print(\"\\n4. Evaluating models...\")\n",
    "    custom_acc, custom_pred, custom_true = evaluate_model(custom_cnn, test_loader)\n",
    "    mobilenet_acc, mobilenet_pred, mobilenet_true = evaluate_model(mobilenet, test_loader)\n",
    "    \n",
    "    # Generate plots\n",
    "    print(\"\\n5. Generating visualizations...\")\n",
    "    plot_training_history(custom_history, \"Custom CNN\")\n",
    "    plot_training_history(mobilenet_history, \"MobileNetV2\")\n",
    "    \n",
    "    plot_confusion_matrix(custom_true, custom_pred, CIFAR10_CLASSES, \"Custom CNN\")\n",
    "    plot_confusion_matrix(mobilenet_true, mobilenet_pred, CIFAR10_CLASSES, \"MobileNetV2\")\n",
    "    \n",
    "    # Return results for analysis\n",
    "    return {\n",
    "        'custom_cnn': {'model': custom_cnn, 'accuracy': custom_acc, 'predictions': custom_pred, 'true': custom_true},\n",
    "        'mobilenet': {'model': mobilenet, 'accuracy': mobilenet_acc, 'predictions': mobilenet_pred, 'true': mobilenet_true}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 8: PERFORMANCE ANALYSIS (4 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def performance_analysis(results):\n",
    "    \"\"\"\n",
    "    Compare models in terms of:\n",
    "    - Test accuracy\n",
    "    - Training stability and convergence  \n",
    "    - Generalization to unseen data\n",
    "    - Trade-offs (complexity vs performance)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TASK 8: PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    custom_acc = results['custom_cnn']['accuracy']\n",
    "    mobilenet_acc = results['mobilenet']['accuracy']\n",
    "    \n",
    "    print(f\"Test Accuracy Comparison:\")\n",
    "    print(f\"  Custom CNN: {custom_acc:.4f}\")\n",
    "    print(f\"  MobileNetV2: {mobilenet_acc:.4f}\")\n",
    "    print(f\"  Difference: {abs(custom_acc - mobilenet_acc):.4f}\")\n",
    "    \n",
    "    # Calculate model parameters\n",
    "    custom_params = sum(p.numel() for p in results['custom_cnn']['model'].parameters())\n",
    "    mobilenet_params = sum(p.numel() for p in results['mobilenet']['model'].parameters())\n",
    "    \n",
    "    print(f\"\\nModel Complexity:\")\n",
    "    print(f\"  Custom CNN: {custom_params:,} parameters\")\n",
    "    print(f\"  MobileNetV2: {mobilenet_params:,} parameters\")\n",
    "    \n",
    "    print(f\"\\nAnalysis:\")\n",
    "    print(f\"- {'MobileNetV2' if mobilenet_acc > custom_acc else 'Custom CNN'} achieved higher accuracy\")\n",
    "    print(f\"- Transfer learning {'did' if mobilenet_acc > custom_acc else 'did not'} outperform custom architecture\")\n",
    "    print(f\"- Parameter efficiency: {custom_params/mobilenet_params:.2f}x ratio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 9: MISCLASSIFIED CASE ANALYSIS (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_misclassified_samples(model, test_loader, model_name, num_samples=8):\n",
    "    \"\"\"\n",
    "    Visualize actual misclassified images to understand model failures.\n",
    "    \n",
    "    This function helps us see what types of images the model struggles with,\n",
    "    providing visual evidence for our analysis of systematic errors.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model to analyze\n",
    "        test_loader: Test data loader\n",
    "        model_name: Name for display purposes\n",
    "        num_samples: Number of misclassified samples to show\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    misclassified_samples = []\n",
    "    \n",
    "    # Collect misclassified samples\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            # Find misclassified samples in this batch\n",
    "            incorrect_mask = predicted != target\n",
    "            \n",
    "            for i in range(len(data)):\n",
    "                if incorrect_mask[i] and len(misclassified_samples) < num_samples:\n",
    "                    # Store the misclassified sample with labels\n",
    "                    img = data[i].cpu()\n",
    "                    true_label = target[i].item()\n",
    "                    pred_label = predicted[i].item()\n",
    "                    misclassified_samples.append((img, true_label, pred_label))\n",
    "            \n",
    "            # Stop when we have enough samples\n",
    "            if len(misclassified_samples) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Create visualization\n",
    "    if misclassified_samples:\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "        fig.suptitle(f'Misclassified Samples: {model_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, (img, true_label, pred_label) in enumerate(misclassified_samples):\n",
    "            row, col = i // 4, i % 4\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Denormalize image for proper display\n",
    "            # Reverse the normalization: img = (img - mean) / std\n",
    "            # So: original = img * std + mean\n",
    "            img_denorm = img * torch.tensor(CIFAR10_STD).view(3, 1, 1) + torch.tensor(CIFAR10_MEAN).view(3, 1, 1)\n",
    "            img_denorm = torch.clamp(img_denorm, 0, 1)  # Ensure values are in [0,1]\n",
    "            \n",
    "            # Display image (convert from CHW to HWC format)\n",
    "            ax.imshow(img_denorm.permute(1, 2, 0))\n",
    "            ax.set_title(f'True: {CIFAR10_CLASSES[true_label]}\\nPredicted: {CIFAR10_CLASSES[pred_label]}', \n",
    "                        fontsize=10, pad=10)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide empty subplots if we have fewer than 8 samples\n",
    "        for i in range(len(misclassified_samples), 8):\n",
    "            row, col = i // 4, i % 4\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Displayed {len(misclassified_samples)} misclassified samples for visual analysis.\")\n",
    "    else:\n",
    "        print(\"No misclassified samples found (perfect accuracy - very unlikely!)\")\n",
    "\n",
    "def analyze_misclassifications(results, test_loader):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of misclassified samples including:\n",
    "    - Visual inspection of actual misclassified images\n",
    "    - Statistical analysis of confusion patterns  \n",
    "    - Systematic error identification\n",
    "    \n",
    "    This analysis helps us understand WHY models make certain errors,\n",
    "    which is crucial for model improvement and real-world deployment.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TASK 9: MISCLASSIFICATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        predictions = data['predictions']\n",
    "        true_labels = data['true']\n",
    "        model = data['model']\n",
    "        \n",
    "        print(f\"\\n📊 Analyzing {model_name.upper()} Misclassifications:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Statistical analysis of errors\n",
    "        misclassified = predictions != true_labels\n",
    "        misclassified_indices = np.where(misclassified)[0]\n",
    "        \n",
    "        print(f\"Total misclassified: {np.sum(misclassified)}\")\n",
    "        print(f\"Error rate: {np.sum(misclassified)/len(true_labels):.3f}\")\n",
    "        \n",
    "        # Visualize actual misclassified samples - KEY REQUIREMENT\n",
    "        print(f\"\\n🖼️  Visualizing misclassified samples for {model_name}:\")\n",
    "        visualize_misclassified_samples(model, test_loader, model_name)\n",
    "        \n",
    "        # Analyze confusion patterns\n",
    "        cm = confusion_matrix(true_labels, predictions)\n",
    "        \n",
    "        # Find most confused class pairs\n",
    "        confused_pairs = []\n",
    "        for i in range(len(CIFAR10_CLASSES)):\n",
    "            for j in range(len(CIFAR10_CLASSES)):\n",
    "                if i != j and cm[i, j] > 0:\n",
    "                    confused_pairs.append((CIFAR10_CLASSES[i], CIFAR10_CLASSES[j], cm[i, j]))\n",
    "        \n",
    "        # Sort by confusion frequency\n",
    "        confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        print(f\"\\n📈 Most frequent confusion pairs:\")\n",
    "        for true_class, pred_class, count in confused_pairs[:5]:\n",
    "            print(f\"    {true_class} → {pred_class}: {count} cases\")\n",
    "        \n",
    "        # Analysis of systematic patterns\n",
    "        print(f\"\\n🔍 Systematic Error Analysis:\")\n",
    "        print(\"    Common error patterns observed:\")\n",
    "        \n",
    "        # Analyze if certain classes are systematically harder\n",
    "        class_error_rates = {}\n",
    "        for i, class_name in enumerate(CIFAR10_CLASSES):\n",
    "            class_mask = true_labels == i\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_errors = np.sum(misclassified[class_mask])\n",
    "                class_total = np.sum(class_mask)\n",
    "                error_rate = class_errors / class_total\n",
    "                class_error_rates[class_name] = error_rate\n",
    "        \n",
    "        # Sort by error rate\n",
    "        sorted_errors = sorted(class_error_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"    Classes ranked by difficulty (error rate):\")\n",
    "        for class_name, error_rate in sorted_errors[:5]:\n",
    "            print(f\"      {class_name}: {error_rate:.3f}\")\n",
    "        \n",
    "        print(f\"\\n💡 Insights for {model_name}:\")\n",
    "        print(\"    - Look for visually similar classes in confusion pairs\")\n",
    "        print(\"    - Consider if certain object orientations cause issues\")  \n",
    "        print(\"    - Check if background complexity affects classification\")\n",
    "        print(\"    - Analyze if small objects are harder to classify\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215327d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 10: EFFICIENCY COMMENTARY (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def efficiency_analysis(results):\n",
    "    \"\"\"\n",
    "    Analyze efficiency in terms of:\n",
    "    - Model size (parameters)\n",
    "    - Inference speed\n",
    "    - Suitability for edge devices/real-time applications\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TASK 10: EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        model = data['model']\n",
    "        \n",
    "        # Parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Model size in MB (assuming float32)\n",
    "        model_size_mb = total_params * 4 / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} Efficiency Metrics:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size: {model_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Inference speed test\n",
    "        model.eval()\n",
    "        dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "        \n",
    "        # Warm up\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        # Time inference\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(100):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_inference_time = (end_time - start_time) / 100 * 1000  # ms\n",
    "        print(f\"  Average inference time: {avg_inference_time:.2f} ms\")\n",
    "        \n",
    "    print(f\"\\nDeployment Considerations:\")\n",
    "    print(f\"- Custom CNN: Smaller, faster, good for edge devices\")\n",
    "    print(f\"- MobileNetV2: Larger but more accurate, suitable for servers/cloud\")\n",
    "    print(f\"- Real-time applications: Both capable of real-time inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f009426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION INSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTION INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "To run the complete assignment:\n",
    "\n",
    "1. Execute all cells above to set up the environment\n",
    "2. Run the main experiment:\n",
    "   results = run_experiment()\n",
    "\n",
    "3. Run the analysis sections:\n",
    "   performance_analysis(results)\n",
    "   analyze_misclassifications(results) \n",
    "   efficiency_analysis(results)\n",
    "\n",
    "This will complete all 10 tasks and generate HD-level results.\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
