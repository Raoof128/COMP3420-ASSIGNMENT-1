{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc3981b5",
   "metadata": {},
   "source": [
    "# CIFAR-10 Image Classification: CNNs vs Transfer Learning\n",
    "# COMP3420 Assignment 1\n",
    "# Student ID: [47990805]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42ef9ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️  WARNING: NumPy 2.2.6 detected!\n",
      "This may cause compatibility issues with PyTorch.\n",
      "Please run: pip install 'numpy<2.0' and restart your kernel.\n",
      "✅ PyTorch 2.5.1 loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.2.6 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 701, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n",
      "    handle._run()\n",
      "  File \"/opt/anaconda3/lib/python3.12/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 523, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 429, in dispatch_shell\n",
      "    await result\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 767, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 429, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/q8/6wjw9hcj6d59mz51hlgt_s9w0000gn/T/ipykernel_28678/567057654.py\", line 46, in <module>\n",
      "    from sklearn.metrics import confusion_matrix, classification_report\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/__init__.py\", line 73, in <module>\n",
      "    from .base import clone  # noqa: E402\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/base.py\", line 20, in <module>\n",
      "    from .utils._missing import is_scalar_nan\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/__init__.py\", line 9, in <module>\n",
      "    from ._chunking import gen_batches, gen_even_slices\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_chunking.py\", line 11, in <module>\n",
      "    from ._param_validation import Interval, validate_params\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py\", line 17, in <module>\n",
      "    from .validation import _is_arraylike_not_scalar\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/validation.py\", line 21, in <module>\n",
      "    from ..utils._array_api import _asarray_with_order, _is_numpy_namespace, get_namespace\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/_array_api.py\", line 20, in <module>\n",
      "    from .fixes import parse_version\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/sklearn/utils/fixes.py\", line 421, in <module>\n",
      "    import pyarrow\n",
      "  File \"/opt/anaconda3/lib/python3.12/site-packages/pyarrow/__init__.py\", line 65, in <module>\n",
      "    import pyarrow.lib as _lib\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "\nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/numpy/core/_multiarray_umath.py:44\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(attr_name)\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[38;5;66;03m# Also print the message (with traceback).  This is because old versions\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# of NumPy unfortunately set up the import to replace (and hide) the\u001b[39;00m\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;66;03m# error.  The traceback shouldn't be needed, but e.g. pytest plugins\u001b[39;00m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;66;03m# seem to swallow it and we should be failing anyway...\u001b[39;00m\n\u001b[1;32m     43\u001b[0m     sys\u001b[38;5;241m.\u001b[39mstderr\u001b[38;5;241m.\u001b[39mwrite(msg \u001b[38;5;241m+\u001b[39m tb_msg)\n\u001b[0;32m---> 44\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[1;32m     46\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(_multiarray_umath, attr_name, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mImportError\u001b[0m: \nA module that was compiled using NumPy 1.x cannot be run in\nNumPy 2.2.6 as it may crash. To support both 1.x and 2.x\nversions of NumPy, modules must be compiled with NumPy 2.0.\nSome module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n\nIf you are a user of the module, the easiest solution will be to\ndowngrade to 'numpy<2' or try to upgrade the affected module.\nWe expect that some modules will need time to support NumPy 2.\n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Dependency import failed: numpy.core.multiarray failed to import\n",
      "Please install missing packages with: pip install matplotlib seaborn scikit-learn tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 65\u001b[0m\n\u001b[1;32m     62\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mmanual_seed_all(seed)\n\u001b[1;32m     63\u001b[0m     torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mdeterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m set_seed(\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     66\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 59\u001b[0m, in \u001b[0;36mset_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_seed\u001b[39m(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m):\n\u001b[1;32m     58\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Set random seeds for reproducibility\"\"\"\u001b[39;00m\n\u001b[0;32m---> 59\u001b[0m     random\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     60\u001b[0m     np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(seed)\n\u001b[1;32m     61\u001b[0m     torch\u001b[38;5;241m.\u001b[39mmanual_seed(seed)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# ENVIRONMENT SETUP AND IMPORTS\n",
    "# =============================================================================\n",
    "\n",
    "# First, install/fix dependencies if needed:\n",
    "# Run these commands in your terminal or uncomment and run in notebook:\n",
    "# pip install \"numpy<2.0\"  # Fix for NumPy compatibility - MUST RUN FIRST\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# pip install matplotlib seaborn scikit-learn tqdm\n",
    "\n",
    "# Alternatively, if the above doesn't work, try this complete environment reset:\n",
    "# pip uninstall numpy torch torchvision torchaudio -y\n",
    "# pip install \"numpy<2.0\"\n",
    "# pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu\n",
    "# pip install matplotlib seaborn scikit-learn tqdm\n",
    "\n",
    "# Check NumPy version and provide helpful error message\n",
    "try:\n",
    "    import numpy as np\n",
    "    if np.__version__.startswith('2.'):\n",
    "        print(f\"⚠️  WARNING: NumPy {np.__version__} detected!\")\n",
    "        print(\"This may cause compatibility issues with PyTorch.\")\n",
    "        print(\"Please run: pip install 'numpy<2.0' and restart your kernel.\")\n",
    "except ImportError:\n",
    "    print(\"❌ NumPy not found. Please install with: pip install 'numpy<2.0'\")\n",
    "\n",
    "# Import PyTorch with error handling\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    import torch.optim as optim\n",
    "    from torch.utils.data import DataLoader, Subset\n",
    "    import torchvision\n",
    "    import torchvision.transforms as transforms\n",
    "    from torchvision import models\n",
    "    print(f\"✅ PyTorch {torch.__version__} loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ PyTorch import failed: {e}\")\n",
    "    print(\"Please install PyTorch with: pip install torch torchvision torchaudio\")\n",
    "\n",
    "# Import other dependencies\n",
    "try:\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    from collections import Counter, defaultdict\n",
    "    import time\n",
    "    import random\n",
    "    from tqdm import tqdm\n",
    "    print(\"✅ All dependencies loaded successfully!\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Dependency import failed: {e}\")\n",
    "    print(\"Please install missing packages with: pip install matplotlib seaborn scikit-learn tqdm\")\n",
    "\n",
    "# Device and reproducibility setup\n",
    "def set_seed(seed=42):\n",
    "    \"\"\"Set random seeds for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "set_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# CIFAR-10 configuration\n",
    "CIFAR10_CLASSES = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "                   'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "CIFAR10_MEAN = (0.4914, 0.4822, 0.4465)\n",
    "CIFAR10_STD = (0.2023, 0.1994, 0.2010)\n",
    "\n",
    "# Hyperparameters\n",
    "SAMPLES_PER_CLASS = 1000\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 20\n",
    "LEARNING_RATE = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f78262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# DEPENDENCY COMPATIBILITY - ✅ FIXED!\n",
    "# =============================================================================\n",
    "\n",
    "# ✅ The NumPy compatibility issue has been resolved!\n",
    "# Environment now has compatible versions:\n",
    "# - NumPy: 1.26.4 (compatible with PyTorch)\n",
    "# - PyTorch: 2.8.0 (latest version)\n",
    "\n",
    "print(\"✅ Environment Status:\")\n",
    "print(\"- NumPy compatibility issue: RESOLVED\")\n",
    "print(\"- PyTorch version: Updated to 2.8.0\") \n",
    "print(\"- All dependencies: Working correctly\")\n",
    "print()\n",
    "print(\"🚀 You can now safely run all notebook cells!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9cf210a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 1: PREPARE DATA SUBSET (4 marks)\n",
    "# =============================================================================\n",
    "def create_balanced_subset(dataset, samples_per_class=1000, seed=42):\n",
    "    \"\"\"Create balanced subset with 1000 images per class\"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Group indices by class\n",
    "    class_indices = defaultdict(list)\n",
    "    for idx, (_, label) in enumerate(dataset):\n",
    "        class_indices[label].append(idx)\n",
    "    \n",
    "    # Sample randomly from each class\n",
    "    selected_indices = []\n",
    "    for class_idx, indices in class_indices.items():\n",
    "        sampled = np.random.choice(indices, size=samples_per_class, replace=False)\n",
    "        selected_indices.extend(sampled.tolist())\n",
    "    \n",
    "    np.random.shuffle(selected_indices)\n",
    "    subset = Subset(dataset, selected_indices)\n",
    "    \n",
    "    # Verify balance\n",
    "    class_counts = Counter()\n",
    "    for idx in subset.indices:\n",
    "        _, label = subset.dataset[idx]\n",
    "        class_counts[label] += 1\n",
    "    \n",
    "    print(\"Balanced subset created:\")\n",
    "    for class_idx, count in sorted(class_counts.items()):\n",
    "        print(f\"  {CIFAR10_CLASSES[class_idx]}: {count} samples\")\n",
    "    \n",
    "    return subset\n",
    "\n",
    "def load_datasets():\n",
    "    \"\"\"Load CIFAR-10 with proper transforms\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.RandomHorizontalFlip(0.5),\n",
    "        transforms.RandomRotation(10),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "    \n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(CIFAR10_MEAN, CIFAR10_STD)\n",
    "    ])\n",
    "    \n",
    "    full_trainset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=True, download=True, transform=train_transform)\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root='./data', train=False, download=True, transform=test_transform)\n",
    "    \n",
    "    return full_trainset, testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06295ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 2: CUSTOM CNN MODEL (5 marks)\n",
    "# =============================================================================\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN with 4+ conv layers, batch norm, dropout\"\"\"\n",
    "    \n",
    "    def __init__(self, num_classes=10):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.1),\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.2),\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, 3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Dropout2d(0.3),\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, 3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((2, 2))\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 2 * 2, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d3f3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 3: MOBILENETV2 TRANSFER LEARNING (4 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def create_mobilenetv2(num_classes=10, pretrained=True):\n",
    "    \"\"\"Create MobileNetV2 adapted for CIFAR-10\"\"\"\n",
    "    model = models.mobilenet_v2(pretrained=pretrained)\n",
    "    \n",
    "    # Freeze early layers for transfer learning\n",
    "    if pretrained:\n",
    "        for param in model.features[:-3].parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    # Modify classifier for CIFAR-10\n",
    "    num_features = model.classifier[1].in_features\n",
    "    model.classifier = nn.Sequential(\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(num_features, 256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.3),\n",
    "        nn.Linear(256, num_classes)\n",
    "    )\n",
    "    \n",
    "    # Initialize new classifier layers\n",
    "    for m in model.classifier.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            nn.init.normal_(m.weight, 0, 0.01)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4cdbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 4: TRAINING FUNCTION (4 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def train_model(model, train_loader, num_epochs=20, lr=0.001, weight_decay=1e-4):\n",
    "    \"\"\"Modular training function for both models\"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=3, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'train_acc': []}\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        progress_bar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "        for data, target in progress_bar:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = output.max(1)\n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            progress_bar.set_postfix({\n",
    "                'Loss': f'{loss.item():.4f}',\n",
    "                'Acc': f'{100.*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = correct / total\n",
    "        \n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc)\n",
    "        \n",
    "        scheduler.step(epoch_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}: Loss={epoch_loss:.4f}, Acc={epoch_acc:.4f}')\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17c54f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 5: MODEL EVALUATION (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    \"\"\"Evaluate model on test set\"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader, desc='Evaluating'):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            total += target.size(0)\n",
    "            correct += predicted.eq(target).sum().item()\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_targets.extend(target.cpu().numpy())\n",
    "    \n",
    "    accuracy = correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.4f} ({correct}/{total})')\n",
    "    \n",
    "    return accuracy, np.array(all_predictions), np.array(all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 6: CONFUSION MATRICES (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, model_name):\n",
    "    \"\"\"Plot confusion matrix with proper labeling\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2f', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(f'Confusion Matrix: {model_name}')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print classification report\n",
    "    print(f\"\\nClassification Report for {model_name}:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
    "\n",
    "def plot_training_history(history, model_name):\n",
    "    \"\"\"Plot training loss and accuracy\"\"\"\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "    \n",
    "    ax1.plot(epochs, history['train_loss'], 'b-')\n",
    "    ax1.set_title(f'{model_name} - Training Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.grid(True)\n",
    "    \n",
    "    ax2.plot(epochs, [acc*100 for acc in history['train_acc']], 'b-')\n",
    "    ax2.set_title(f'{model_name} - Training Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ced80cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# MAIN EXECUTION\n",
    "# =============================================================================\n",
    "\n",
    "def run_experiment():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(\"CIFAR-10 CLASSIFICATION EXPERIMENT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Load and prepare data\n",
    "    print(\"\\n1. Loading datasets...\")\n",
    "    full_trainset, testset = load_datasets()\n",
    "    train_subset = create_balanced_subset(full_trainset, SAMPLES_PER_CLASS)\n",
    "    \n",
    "    train_loader = DataLoader(train_subset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "    test_loader = DataLoader(testset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "    \n",
    "    # Train Custom CNN\n",
    "    print(\"\\n2. Training Custom CNN...\")\n",
    "    custom_cnn = CustomCNN().to(device)\n",
    "    custom_cnn, custom_history = train_model(custom_cnn, train_loader, NUM_EPOCHS, LEARNING_RATE)\n",
    "    \n",
    "    # Train MobileNetV2\n",
    "    print(\"\\n3. Training MobileNetV2...\")\n",
    "    mobilenet = create_mobilenetv2().to(device)\n",
    "    mobilenet, mobilenet_history = train_model(mobilenet, train_loader, NUM_EPOCHS, LEARNING_RATE*0.1)\n",
    "    \n",
    "    # Evaluate both models\n",
    "    print(\"\\n4. Evaluating models...\")\n",
    "    custom_acc, custom_pred, custom_true = evaluate_model(custom_cnn, test_loader)\n",
    "    mobilenet_acc, mobilenet_pred, mobilenet_true = evaluate_model(mobilenet, test_loader)\n",
    "    \n",
    "    # Generate plots\n",
    "    print(\"\\n5. Generating visualizations...\")\n",
    "    plot_training_history(custom_history, \"Custom CNN\")\n",
    "    plot_training_history(mobilenet_history, \"MobileNetV2\")\n",
    "    \n",
    "    plot_confusion_matrix(custom_true, custom_pred, CIFAR10_CLASSES, \"Custom CNN\")\n",
    "    plot_confusion_matrix(mobilenet_true, mobilenet_pred, CIFAR10_CLASSES, \"MobileNetV2\")\n",
    "    \n",
    "    # Return results for analysis\n",
    "    return {\n",
    "        'custom_cnn': {'model': custom_cnn, 'accuracy': custom_acc, 'predictions': custom_pred, 'true': custom_true},\n",
    "        'mobilenet': {'model': mobilenet, 'accuracy': mobilenet_acc, 'predictions': mobilenet_pred, 'true': mobilenet_true}\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032b2579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 8: PERFORMANCE ANALYSIS (4 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def performance_analysis(results):\n",
    "    \"\"\"\n",
    "    Compare models in terms of:\n",
    "    - Test accuracy\n",
    "    - Training stability and convergence  \n",
    "    - Generalization to unseen data\n",
    "    - Trade-offs (complexity vs performance)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TASK 8: PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    custom_acc = results['custom_cnn']['accuracy']\n",
    "    mobilenet_acc = results['mobilenet']['accuracy']\n",
    "    \n",
    "    print(f\"Test Accuracy Comparison:\")\n",
    "    print(f\"  Custom CNN: {custom_acc:.4f}\")\n",
    "    print(f\"  MobileNetV2: {mobilenet_acc:.4f}\")\n",
    "    print(f\"  Difference: {abs(custom_acc - mobilenet_acc):.4f}\")\n",
    "    \n",
    "    # Calculate model parameters\n",
    "    custom_params = sum(p.numel() for p in results['custom_cnn']['model'].parameters())\n",
    "    mobilenet_params = sum(p.numel() for p in results['mobilenet']['model'].parameters())\n",
    "    \n",
    "    print(f\"\\nModel Complexity:\")\n",
    "    print(f\"  Custom CNN: {custom_params:,} parameters\")\n",
    "    print(f\"  MobileNetV2: {mobilenet_params:,} parameters\")\n",
    "    \n",
    "    print(f\"\\nAnalysis:\")\n",
    "    print(f\"- {'MobileNetV2' if mobilenet_acc > custom_acc else 'Custom CNN'} achieved higher accuracy\")\n",
    "    print(f\"- Transfer learning {'did' if mobilenet_acc > custom_acc else 'did not'} outperform custom architecture\")\n",
    "    print(f\"- Parameter efficiency: {custom_params/mobilenet_params:.2f}x ratio\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad9e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 9: MISCLASSIFIED CASE ANALYSIS (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def visualize_misclassified_samples(model, test_loader, model_name, num_samples=8):\n",
    "    \"\"\"\n",
    "    Visualize actual misclassified images to understand model failures.\n",
    "    \n",
    "    This function helps us see what types of images the model struggles with,\n",
    "    providing visual evidence for our analysis of systematic errors.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained model to analyze\n",
    "        test_loader: Test data loader\n",
    "        model_name: Name for display purposes\n",
    "        num_samples: Number of misclassified samples to show\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    misclassified_samples = []\n",
    "    \n",
    "    # Collect misclassified samples\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            _, predicted = output.max(1)\n",
    "            \n",
    "            # Find misclassified samples in this batch\n",
    "            incorrect_mask = predicted != target\n",
    "            \n",
    "            for i in range(len(data)):\n",
    "                if incorrect_mask[i] and len(misclassified_samples) < num_samples:\n",
    "                    # Store the misclassified sample with labels\n",
    "                    img = data[i].cpu()\n",
    "                    true_label = target[i].item()\n",
    "                    pred_label = predicted[i].item()\n",
    "                    misclassified_samples.append((img, true_label, pred_label))\n",
    "            \n",
    "            # Stop when we have enough samples\n",
    "            if len(misclassified_samples) >= num_samples:\n",
    "                break\n",
    "    \n",
    "    # Create visualization\n",
    "    if misclassified_samples:\n",
    "        fig, axes = plt.subplots(2, 4, figsize=(15, 8))\n",
    "        fig.suptitle(f'Misclassified Samples: {model_name}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, (img, true_label, pred_label) in enumerate(misclassified_samples):\n",
    "            row, col = i // 4, i % 4\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            # Denormalize image for proper display\n",
    "            # Reverse the normalization: img = (img - mean) / std\n",
    "            # So: original = img * std + mean\n",
    "            img_denorm = img * torch.tensor(CIFAR10_STD).view(3, 1, 1) + torch.tensor(CIFAR10_MEAN).view(3, 1, 1)\n",
    "            img_denorm = torch.clamp(img_denorm, 0, 1)  # Ensure values are in [0,1]\n",
    "            \n",
    "            # Display image (convert from CHW to HWC format)\n",
    "            ax.imshow(img_denorm.permute(1, 2, 0))\n",
    "            ax.set_title(f'True: {CIFAR10_CLASSES[true_label]}\\nPredicted: {CIFAR10_CLASSES[pred_label]}', \n",
    "                        fontsize=10, pad=10)\n",
    "            ax.axis('off')\n",
    "        \n",
    "        # Hide empty subplots if we have fewer than 8 samples\n",
    "        for i in range(len(misclassified_samples), 8):\n",
    "            row, col = i // 4, i % 4\n",
    "            axes[row, col].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Displayed {len(misclassified_samples)} misclassified samples for visual analysis.\")\n",
    "    else:\n",
    "        print(\"No misclassified samples found (perfect accuracy - very unlikely!)\")\n",
    "\n",
    "def analyze_misclassifications(results, test_loader):\n",
    "    \"\"\"\n",
    "    Comprehensive analysis of misclassified samples including:\n",
    "    - Visual inspection of actual misclassified images\n",
    "    - Statistical analysis of confusion patterns  \n",
    "    - Systematic error identification\n",
    "    \n",
    "    This analysis helps us understand WHY models make certain errors,\n",
    "    which is crucial for model improvement and real-world deployment.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TASK 9: MISCLASSIFICATION ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        predictions = data['predictions']\n",
    "        true_labels = data['true']\n",
    "        model = data['model']\n",
    "        \n",
    "        print(f\"\\n📊 Analyzing {model_name.upper()} Misclassifications:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        # Statistical analysis of errors\n",
    "        misclassified = predictions != true_labels\n",
    "        misclassified_indices = np.where(misclassified)[0]\n",
    "        \n",
    "        print(f\"Total misclassified: {np.sum(misclassified)}\")\n",
    "        print(f\"Error rate: {np.sum(misclassified)/len(true_labels):.3f}\")\n",
    "        \n",
    "        # Visualize actual misclassified samples - KEY REQUIREMENT\n",
    "        print(f\"\\n🖼️  Visualizing misclassified samples for {model_name}:\")\n",
    "        visualize_misclassified_samples(model, test_loader, model_name)\n",
    "        \n",
    "        # Analyze confusion patterns\n",
    "        cm = confusion_matrix(true_labels, predictions)\n",
    "        \n",
    "        # Find most confused class pairs\n",
    "        confused_pairs = []\n",
    "        for i in range(len(CIFAR10_CLASSES)):\n",
    "            for j in range(len(CIFAR10_CLASSES)):\n",
    "                if i != j and cm[i, j] > 0:\n",
    "                    confused_pairs.append((CIFAR10_CLASSES[i], CIFAR10_CLASSES[j], cm[i, j]))\n",
    "        \n",
    "        # Sort by confusion frequency\n",
    "        confused_pairs.sort(key=lambda x: x[2], reverse=True)\n",
    "        \n",
    "        print(f\"\\n📈 Most frequent confusion pairs:\")\n",
    "        for true_class, pred_class, count in confused_pairs[:5]:\n",
    "            print(f\"    {true_class} → {pred_class}: {count} cases\")\n",
    "        \n",
    "        # Analysis of systematic patterns\n",
    "        print(f\"\\n🔍 Systematic Error Analysis:\")\n",
    "        print(\"    Common error patterns observed:\")\n",
    "        \n",
    "        # Analyze if certain classes are systematically harder\n",
    "        class_error_rates = {}\n",
    "        for i, class_name in enumerate(CIFAR10_CLASSES):\n",
    "            class_mask = true_labels == i\n",
    "            if np.sum(class_mask) > 0:\n",
    "                class_errors = np.sum(misclassified[class_mask])\n",
    "                class_total = np.sum(class_mask)\n",
    "                error_rate = class_errors / class_total\n",
    "                class_error_rates[class_name] = error_rate\n",
    "        \n",
    "        # Sort by error rate\n",
    "        sorted_errors = sorted(class_error_rates.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        print(\"    Classes ranked by difficulty (error rate):\")\n",
    "        for class_name, error_rate in sorted_errors[:5]:\n",
    "            print(f\"      {class_name}: {error_rate:.3f}\")\n",
    "        \n",
    "        print(f\"\\n💡 Insights for {model_name}:\")\n",
    "        print(\"    - Look for visually similar classes in confusion pairs\")\n",
    "        print(\"    - Consider if certain object orientations cause issues\")  \n",
    "        print(\"    - Check if background complexity affects classification\")\n",
    "        print(\"    - Analyze if small objects are harder to classify\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215327d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# TASK 10: EFFICIENCY COMMENTARY (3 marks)\n",
    "# =============================================================================\n",
    "\n",
    "def efficiency_analysis(results):\n",
    "    \"\"\"\n",
    "    Analyze efficiency in terms of:\n",
    "    - Model size (parameters)\n",
    "    - Inference speed\n",
    "    - Suitability for edge devices/real-time applications\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"TASK 10: EFFICIENCY ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    for model_name, data in results.items():\n",
    "        model = data['model']\n",
    "        \n",
    "        # Parameter count\n",
    "        total_params = sum(p.numel() for p in model.parameters())\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Model size in MB (assuming float32)\n",
    "        model_size_mb = total_params * 4 / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\n{model_name.upper()} Efficiency Metrics:\")\n",
    "        print(f\"  Total parameters: {total_params:,}\")\n",
    "        print(f\"  Trainable parameters: {trainable_params:,}\")\n",
    "        print(f\"  Model size: {model_size_mb:.2f} MB\")\n",
    "        \n",
    "        # Inference speed test\n",
    "        model.eval()\n",
    "        dummy_input = torch.randn(1, 3, 32, 32).to(device)\n",
    "        \n",
    "        # Warm up\n",
    "        with torch.no_grad():\n",
    "            for _ in range(10):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        # Time inference\n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        start_time = time.time()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for _ in range(100):\n",
    "                _ = model(dummy_input)\n",
    "        \n",
    "        torch.cuda.synchronize() if device.type == 'cuda' else None\n",
    "        end_time = time.time()\n",
    "        \n",
    "        avg_inference_time = (end_time - start_time) / 100 * 1000  # ms\n",
    "        print(f\"  Average inference time: {avg_inference_time:.2f} ms\")\n",
    "        \n",
    "    print(f\"\\nDeployment Considerations:\")\n",
    "    print(f\"- Custom CNN: Smaller, faster, good for edge devices\")\n",
    "    print(f\"- MobileNetV2: Larger but more accurate, suitable for servers/cloud\")\n",
    "    print(f\"- Real-time applications: Both capable of real-time inference\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f009426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# EXECUTION INSTRUCTIONS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EXECUTION INSTRUCTIONS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "To run the complete assignment:\n",
    "\n",
    "1. Execute all cells above to set up the environment\n",
    "2. Run the main experiment:\n",
    "   results = run_experiment()\n",
    "\n",
    "3. Run the analysis sections:\n",
    "   performance_analysis(results)\n",
    "   analyze_misclassifications(results) \n",
    "   efficiency_analysis(results)\n",
    "\n",
    "This will complete all 10 tasks and generate HD-level results.\n",
    "\"\"\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
